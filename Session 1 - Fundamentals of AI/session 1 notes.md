
Deep Learning & NN Field is about. We will constantly keep coming back to this.
1. Data & its representation. 
  1. $\Large batch=(X_{collected}, Y_{observed})$
  2. $\Large Data \rightarrow Information \rightarrow Knowledge \rightarrow Understanding \rightarrow Wisdom$
  3. Compress, Dense, Fundamental Patterns Extraction, Generalization
2. Neural Network & it's parameters $\Large W$. as Deep Function Approximator & Mathematical Learning
  1. $\Large model = f(X,W_{[layer\_no][neuron\_no]})$
  2. $\Large Y_{predicted} = f(X_{collected},W_{[layer\_no][neuron\_no]})$
  3. $\Large error\_value = error\_func(Y_{predicted}, Y_{observed})$
  4. $\Large minimize(error\_value)$
3. Hardware running the NN


Receptive Field of Image & Attention


Experiment
1. Image Data, Matrix & Tensor (Sympy + Latex + Pytorch)
2. Channels of Data (torch.randn()) (Pytorch)
3. Layers = Transformations (PYTORCH)
   1. Input of transform
   2. Output of transform (Geometry + Matrix Multiplication + Latex Tikz & Latex + Javascript webpage)
   3. Parameters in Layer
      1. Parameter count 
      2. Parameter memory
   4. Flops Operations
   5. Transform Layers, what they are each doing list
4. DIKW Successive Transformations & Pytorch transformations
   1. Pytorch model as sequential
5.  Types of Layers. Conv, FC (PYTORCH)
   1. [Channel Notes & Experiments colab](https://colab.research.google.com/drive/1cKqF4fO5eWTOITeeFfe5Oy0fERGmiuML)
6. Python Memory Profile & Python Execution Profile

